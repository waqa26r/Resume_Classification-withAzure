{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Azure Cognitive Services credentials\n",
    "subscription_key = 'YOUR_SUBSCRIPTION_KEY'\n",
    "endpoint = 'YOUR_ENDPOINT'\n",
    "\n",
    "# Set up storage container details\n",
    "storage_account_name = 'YOUR_STORAGE_ACCOUNT_NAME'\n",
    "storage_account_key = 'YOUR_STORAGE_ACCOUNT_KEY'\n",
    "container_name = 'YOUR_CONTAINER_NAME'\n",
    "\n",
    "# Set up Excel output file\n",
    "excel_file_path = 'output.xlsx'\n",
    "\n",
    "# Define the skills and their corresponding classes\n",
    "skill_classes = {\n",
    "    'web development': ['html', 'css', 'javascript'],\n",
    "    'data science': ['python', 'r'],\n",
    "    'backend development': ['java', 'c#', 'node.js']\n",
    "}\n",
    "\n",
    "# Function to extract text from PDF files using Azure Cognitive Services OCR API\n",
    "def extract_text_from_pdf(file_path):\n",
    "    url = f\"https://{storage_account_name}.blob.core.windows.net/{container_name}/{file_path}\"\n",
    "    headers = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "    params = {'language': 'en', 'detectOrientation': 'true'}\n",
    "    response = requests.post(\n",
    "        f\"{endpoint}/vision/v3.2/read/analyze\",\n",
    "        headers=headers,\n",
    "        params=params,\n",
    "        json={\"url\": url}\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    operation_url = response.headers[\"Operation-Location\"]\n",
    "\n",
    "    analysis = {}\n",
    "    while not \"succeeded\" in analysis:\n",
    "        response_final = requests.get(\n",
    "            response.headers[\"Operation-Location\"], headers=headers\n",
    "        )\n",
    "        analysis = response_final.json()\n",
    "\n",
    "    extracted_text = \"\"\n",
    "    for result in analysis[\"analyzeResult\"][\"readResults\"]:\n",
    "        for line in result[\"lines\"]:\n",
    "            extracted_text += line[\"text\"] + \" \"\n",
    "    return extracted_text\n",
    "\n",
    "# Function to classify the skills based on the extracted text\n",
    "def classify_skills(text):\n",
    "    skills = []\n",
    "    for skill_class, skill_list in skill_classes.items():\n",
    "        for skill in skill_list:\n",
    "            if skill in text:\n",
    "                skills.append(skill_class)\n",
    "                break\n",
    "    return skills\n",
    "\n",
    "# Function to process PDF files, extract text, and classify skills\n",
    "def process_pdf_files():\n",
    "    pdf_files = ['file1.pdf', 'file2.pdf', 'file3.pdf', 'file4.pdf']\n",
    "    results = []\n",
    "\n",
    "    for file_path in pdf_files:\n",
    "        # Extract text from PDF\n",
    "        extracted_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "        # Classify skills\n",
    "        skills = classify_skills(extracted_text)\n",
    "\n",
    "        # Create result entry\n",
    "        result = {'file': file_path, 'skills': ', '.join(skills)}\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to write results to Excel file\n",
    "def write_to_excel(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(excel_file_path, index=False)\n",
    "    print(f\"Results written to {excel_file_path}\")\n",
    "\n",
    "# Main function to execute the workflow\n",
    "def main():\n",
    "    # Create a temporary directory to store PDF files locally\n",
    "    os.makedirs('temp', exist_ok=True)\n",
    "    os.chdir('temp')\n",
    "\n",
    "    # Download PDF files from Azure Storage Container\n",
    "    for file_path in pdf_files:\n",
    "        os.system(f'azcopy copy \"https://{storage_account_name}.blob.core.windows.net/{container_name}/{file_path}\" .')\n",
    "\n",
    "    # Process PDF files, extract text, and classify skills\n",
    "    results = process_pdf_files()\n",
    "\n",
    "    # Write results to Excel file\n",
    "    write_to_excel(results)\n",
    "\n",
    "    # Clean up temporary directory\n",
    "    os.chdir('..')\n",
    "    os.system('rm -rf temp')\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Azure Cognitive Services credentials\n",
    "subscription_key = '68446312c2fa4a0c81877d34cc598748'\n",
    "endpoint = 'https://docclasification1.cognitiveservices.azure.com/'\n",
    "\n",
    "# Set up storage container details\n",
    "storage_account_name = 'democontainer1'\n",
    "storage_account_key = 'aOJFYVktRMigkU+6MBTmG+SARhkL4Rs9TjqNUy9C7FdH97JbYeKa2mW5DnOILnPtqYD9uFhE/HmF+AStuHh+/Q=='\n",
    "container_name = 'democonatainer1'\n",
    "\n",
    "# Set up Excel output file\n",
    "excel_file_path = 'output.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the skills and their corresponding classes\n",
    "skill_classes = {\n",
    "    'web development': ['html', 'css', 'javascript'],\n",
    "    'data science': ['python', 'r'],\n",
    "    'backend development': ['java', 'c#', 'node.js']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDF files using Azure Cognitive Services OCR API\n",
    "def extract_text_from_pdf(file_path):\n",
    "    url = f\"https://{storage_account_name}.blob.core.windows.net/{container_name}/{file_path}\"\n",
    "    headers = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "    params = {'language': 'en', 'detectOrientation': 'true'}\n",
    "    response = requests.post(\n",
    "        f\"{endpoint}/vision/v3.2/read/analyze\",\n",
    "        headers=headers,\n",
    "        params=params,\n",
    "        json={\"url\": url}\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    operation_url = response.headers[\"Operation-Location\"]\n",
    "\n",
    "    analysis = {}\n",
    "    while not \"succeeded\" in analysis:\n",
    "        response_final = requests.get(\n",
    "            response.headers[\"Operation-Location\"], headers=headers\n",
    "        )\n",
    "        analysis = response_final.json()\n",
    "\n",
    "    extracted_text = \"\"\n",
    "    for result in analysis[\"analyzeResult\"][\"readResults\"]:\n",
    "        for line in result[\"lines\"]:\n",
    "            extracted_text += line[\"text\"] + \" \"\n",
    "    return extracted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify the skills based on the extracted text\n",
    "def classify_skills(text):\n",
    "    skills = []\n",
    "    for skill_class, skill_list in skill_classes.items():\n",
    "        for skill in skill_list:\n",
    "            if skill in text:\n",
    "                skills.append(skill_class)\n",
    "                break\n",
    "    return skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process PDF files, extract text, and classify skills\n",
    "def process_pdf_files():\n",
    "    pdf_files = ['file1.pdf', 'file2.pdf', 'file3.pdf', 'file4.pdf']\n",
    "    results = []\n",
    "\n",
    "    for file_path in pdf_files:\n",
    "        # Extract text from PDF\n",
    "        extracted_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "        # Classify skills\n",
    "        skills = classify_skills(extracted_text)\n",
    "\n",
    "        # Create result entry\n",
    "        result = {'file': file_path, 'skills': ', '.join(skills)}\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write results to Excel file\n",
    "def write_to_excel(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(excel_file_path, index=False)\n",
    "    print(f\"Results written to {excel_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://docclasification1.cognitiveservices.azure.com//vision/v3.2/read/analyze?language=en&detectOrientation=true",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m# Run the main function\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 24\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     os\u001b[39m.\u001b[39msystem(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mazcopy copy \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://\u001b[39m\u001b[39m{\u001b[39;00mstorage_account_name\u001b[39m}\u001b[39;00m\u001b[39m.blob.core.windows.net/\u001b[39m\u001b[39m{\u001b[39;00mcontainer_name\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m .\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Process PDF files, extract text, and classify skills\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m results \u001b[39m=\u001b[39m process_pdf_files()\n\u001b[0;32m     15\u001b[0m \u001b[39m# Write results to Excel file\u001b[39;00m\n\u001b[0;32m     16\u001b[0m write_to_excel(results)\n",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m, in \u001b[0;36mprocess_pdf_files\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m file_path \u001b[39min\u001b[39;00m pdf_files:\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Extract text from PDF\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     extracted_text \u001b[39m=\u001b[39m extract_text_from_pdf(file_path)\n\u001b[0;32m     10\u001b[0m     \u001b[39m# Classify skills\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     skills \u001b[39m=\u001b[39m classify_skills(extracted_text)\n",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m, in \u001b[0;36mextract_text_from_pdf\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlanguage\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdetectOrientation\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m      6\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(\n\u001b[0;32m      7\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mendpoint\u001b[39m}\u001b[39;00m\u001b[39m/vision/v3.2/read/analyze\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m      9\u001b[0m     params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m     10\u001b[0m     json\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m: url}\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[0;32m     13\u001b[0m operation_url \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mheaders[\u001b[39m\"\u001b[39m\u001b[39mOperation-Location\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     15\u001b[0m analysis \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[39m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m{\u001b[39;00mreason\u001b[39m}\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://docclasification1.cognitiveservices.azure.com//vision/v3.2/read/analyze?language=en&detectOrientation=true"
     ]
    }
   ],
   "source": [
    "# Main function to execute the workflow\n",
    "def main():\n",
    "    # Create a temporary directory to store PDF files locally\n",
    "    os.makedirs('temp', exist_ok=True)\n",
    "    os.chdir('temp')\n",
    "\n",
    "    # Download PDF files from Azure Storage Container\n",
    "    pdf_files = ['file1.pdf', 'file2.pdf', 'file3.pdf', 'file4.pdf']\n",
    "    for file_path in pdf_files:\n",
    "        os.system(f'azcopy copy \"https://{storage_account_name}.blob.core.windows.net/{container_name}/{file_path}\" .')\n",
    "\n",
    "    # Process PDF files, extract text, and classify skills\n",
    "    results = process_pdf_files()\n",
    "\n",
    "    # Write results to Excel file\n",
    "    write_to_excel(results)\n",
    "\n",
    "    # Clean up temporary directory\n",
    "    os.chdir('..')\n",
    "    os.system('rm -rf temp')\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfreader\n",
      "  Downloading pdfreader-0.1.12.tar.gz (2.9 MB)\n",
      "                                              0.0/2.9 MB ? eta -:--:--\n",
      "     --                                       0.2/2.9 MB 5.3 MB/s eta 0:00:01\n",
      "     --                                       0.2/2.9 MB 3.1 MB/s eta 0:00:01\n",
      "     --                                       0.2/2.9 MB 3.1 MB/s eta 0:00:01\n",
      "     ---                                      0.3/2.9 MB 1.9 MB/s eta 0:00:02\n",
      "     ----                                     0.3/2.9 MB 1.4 MB/s eta 0:00:02\n",
      "     -----                                    0.4/2.9 MB 1.5 MB/s eta 0:00:02\n",
      "     ------                                   0.4/2.9 MB 1.6 MB/s eta 0:00:02\n",
      "     --------                                 0.6/2.9 MB 1.9 MB/s eta 0:00:02\n",
      "     ---------                                0.7/2.9 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------                             0.9/2.9 MB 2.1 MB/s eta 0:00:01\n",
      "     --------------                           1.0/2.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------                          1.1/2.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------                       1.3/2.9 MB 2.5 MB/s eta 0:00:01\n",
      "     --------------------                     1.5/2.9 MB 2.6 MB/s eta 0:00:01\n",
      "     ----------------------                   1.6/2.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------                 1.8/2.9 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------------------------             2.0/2.9 MB 2.9 MB/s eta 0:00:01\n",
      "     -------------------------------          2.2/2.9 MB 3.0 MB/s eta 0:00:01\n",
      "     -------------------------------          2.3/2.9 MB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------------     2.6/2.9 MB 3.2 MB/s eta 0:00:01\n",
      "     --------------------------------------   2.8/2.9 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.9/2.9 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.9/2.9 MB 3.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting bitarray>=1.1.0 (from pdfreader)\n",
      "  Downloading bitarray-2.7.6-cp311-cp311-win_amd64.whl (118 kB)\n",
      "                                              0.0/118.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 118.6/118.6 kB 6.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pdfreader) (9.5.0)\n",
      "Requirement already satisfied: pycryptodome>=3.9.9 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pdfreader) (3.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pdfreader) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.1->pdfreader) (1.12.0)\n",
      "Building wheels for collected packages: pdfreader\n",
      "  Building wheel for pdfreader (setup.py): started\n",
      "  Building wheel for pdfreader (setup.py): finished with status 'done'\n",
      "  Created wheel for pdfreader: filename=pdfreader-0.1.12-py3-none-any.whl size=134571 sha256=eae5de923037b6666cc0a3d8c22045b0d7a5b6df0b40e5bf3e0167c57857784b\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\ae\\2a\\d7\\534c5ec691eece18000ef93a0daedf20821730e771cca259d5\n",
      "Successfully built pdfreader\n",
      "Installing collected packages: bitarray, pdfreader\n",
      "Successfully installed bitarray-2.7.6 pdfreader-0.1.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfreader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "import textract\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Blob Storage configuration\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=democontainer1;AccountKey=aOJFYVktRMigkU+6MBTmG+SARhkL4Rs9TjqNUy9C7FdH97JbYeKa2mW5DnOILnPtqYD9uFhE/HmF+AStuHh+/Q==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"democonatainer1\"\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    \"Web Development\": [\"html\", \"css\", \"javascript\", \"frontend\"],\n",
    "    \"Data Science\": [\"python\", \"machine learning\", \"data analysis\"],\n",
    "    \"Mobile Development\": [\"java\", \"android\", \"ios\"],\n",
    "    # Add more classes and associated keywords as needed\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase and remove non-alphanumeric characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Join tokens back into a single string\n",
    "    processed_text = \" \".join(filtered_tokens)\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        text = textract.process(file_path).decode(\"utf-8\")\n",
    "        return text\n",
    "    except Exception:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"D:\\\\nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "resumes = []\n",
    "\n",
    "blob_list = container_client.list_blobs()\n",
    "for blob in blob_list:\n",
    "    file_name = blob.name\n",
    "    file_path = os.path.join(\"pdf_files\", file_name)\n",
    "    \n",
    "    with open(file_path, \"wb\") as file:\n",
    "        blob_data = container_client.download_blob(blob)\n",
    "        blob_data.readinto(file)\n",
    "    \n",
    "    if file_name.lower().endswith(\".pdf\"):\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in range(len(pdf_reader.pages)):\n",
    "                text += pdf_reader.pages[page].extract_text()\n",
    "    else:\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    \n",
    "    processed_text = preprocess_text(text)\n",
    "    resumes.append(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(resumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 106)\t1\n",
      "  (0, 173)\t1\n",
      "  (0, 404)\t1\n",
      "  (0, 257)\t1\n",
      "  (0, 643)\t5\n",
      "  (0, 999)\t5\n",
      "  (0, 242)\t6\n",
      "  (0, 589)\t1\n",
      "  (0, 224)\t1\n",
      "  (0, 921)\t8\n",
      "  (0, 168)\t1\n",
      "  (0, 197)\t1\n",
      "  (0, 328)\t1\n",
      "  (0, 464)\t1\n",
      "  (0, 456)\t1\n",
      "  (0, 529)\t1\n",
      "  (0, 582)\t2\n",
      "  (0, 920)\t1\n",
      "  (0, 900)\t1\n",
      "  (0, 85)\t3\n",
      "  (0, 960)\t2\n",
      "  (0, 615)\t1\n",
      "  (0, 377)\t2\n",
      "  (0, 385)\t1\n",
      "  (0, 121)\t1\n",
      "  :\t:\n",
      "  (6, 730)\t1\n",
      "  (6, 933)\t1\n",
      "  (6, 593)\t1\n",
      "  (6, 506)\t1\n",
      "  (6, 128)\t1\n",
      "  (6, 713)\t1\n",
      "  (6, 638)\t1\n",
      "  (6, 203)\t1\n",
      "  (6, 209)\t1\n",
      "  (6, 741)\t1\n",
      "  (6, 620)\t1\n",
      "  (6, 552)\t1\n",
      "  (6, 142)\t1\n",
      "  (6, 709)\t1\n",
      "  (6, 799)\t1\n",
      "  (6, 752)\t1\n",
      "  (6, 129)\t1\n",
      "  (6, 937)\t1\n",
      "  (6, 753)\t1\n",
      "  (6, 494)\t1\n",
      "  (6, 932)\t1\n",
      "  (6, 338)\t1\n",
      "  (6, 177)\t1\n",
      "  (6, 820)\t1\n",
      "  (6, 202)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    \"Web Development\": [\"html\", \"css\", \"javascript\", \"frontend\"],\n",
    "    \"Data Science\": [\"python\", \"machine learning\", \"data analysis\"],\n",
    "    \"Mobile Development\": [\"java\", \"android\", \"ios\"],\n",
    "    \"UI/UX Design\": [\"ui\", \"ux\", \"prototyping\"],\n",
    "    # Add more classes and associated keywords as needed\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
